### 基于 Centos8 系统通过 Cephadm 快速部署  Ceph16（pacific）版本

#### 前期准备

##### 1. 安装依赖
lvm因为系统自带的都有，所以就不用单独安装了，
```cassandraql
dnf install epel-release -y
dnf install python3 -y
dnf install podman -y
dnf install -y chrony
systemctl start chronyd && systemctl enable chronyd
```
chrony时间服务为必须安装，具体有2点原因：1为不安装在添加主机的时候会报错，2为即使安装成功ceph -s会也提示时间不同步！
##### 2.关闭防火墙和selinux (每台都执行)
```cassandraql
systemctl disable firewalld && systemctl stop firewalld
setenforce 0
sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
```

##### 3.分别在三个节点设置主机名
```cassandraql
hostnamectl set-hostname ceph01
hostnamectl set-hostname ceph02
hostnamectl set-hostname ceph03
```
重启机器 reboot
cephadm需要主机名为短主机名，不能为FQDN，否者在添加主机会报错！

##### 4.添加hosts文件中的主机名和IP关系，主机名需要和上面一致
172.31.96.70 ceph01  2Cpu 4G内存  20G系统盘，20G数据盘
172.31.96.71 ceph02  2Cpu 4G内存  20G系统盘，20G数据盘
172.31.96.72 ceph03  2Cpu 4G内存  20G系统盘，20G数据盘
```cassandraql
cat >> /etc/hosts <<EOF
172.31.96.70 ceph01 
172.31.96.71 ceph02 
172.31.96.72 ceph03 
EOF
```
 

#### 安装 cephadm
cephadm 命令可以
1. 引导新集群
2. 使用有效的Ceph CLI启动容器化的Shell
3. 帮助调试容器化的Ceph守护进程。
以下操作只在一台节点执行就可以
##### 1.使用curl获取独立脚本的最新版本。网络不好的话可直接去GitHub复制

```cassandraql
curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm
chmod +x cephadm
./cephadm add-repo --release pacific
./cephadm install
./cephadm install  ceph-common
```
官方文档中还提到了另一种安装cephadm方式，就是通过dnf install -y cephadm安装，实践证明最好不要使用这种方式，这种方式安装的cephadm可能不是最新版本的，但cephadm去拉的容器版本又是最新的，会导致两个版本不一致！


 

#### 引导新群集
##### 1.先创建一个目录：/etc/ceph
```cassandraql
mkdir -p /etc/ceph
```

##### 2.运行该命令：ceph bootstrap
```cassandraql
cephadm bootstrap --mon-ip 172.31.96.70
```
此命令将会进行以下操作：
- 为本地主机上的新群集创建monitor和manager守护程序。
- 为 Ceph 群集生成新的 SSH 密钥，并将其添加到root用户的文件/root/.ssh/authorized_keys
- 将与新群集通信所需的最小配置文件保存到 /etc/ceph/ceph.conf
- 将client.admin管理（特权！）密钥的副本写入/etc/ceph/ceph.client.admin.keyring
- 将公钥的副本写入/etc/ceph/ceph.pub

安装日志如下为成功
```cassandraql
[root@ceph01 ~]# cephadm bootstrap --mon-ip 172.31.96.70
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit chronyd.service is enabled and running
Repeating the final host check...
podman (/usr/bin/podman) version 3.3.1 is present
systemctl is present
lvcreate is present
Unit chronyd.service is enabled and running
Host looks OK
Cluster fsid: ba25aef4-19f6-11ed-867d-00163e005933
Verifying IP 172.31.96.71 port 3300 ...
ERROR: [Errno 99] Cannot assign requested address
[root@ceph01 ~]# cephadm bootstrap --mon-ip 172.31.96.70
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit chronyd.service is enabled and running
Repeating the final host check...
podman (/usr/bin/podman) version 3.3.1 is present
systemctl is present
lvcreate is present
Unit chronyd.service is enabled and running
Host looks OK
Cluster fsid: c32ff766-19f6-11ed-aa17-00163e005933
Verifying IP 172.31.96.70 port 3300 ...
Verifying IP 172.31.96.70 port 6789 ...
Mon IP `172.31.96.70` is in CIDR network `172.31.96.0/20`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Assimilating anything we can from ceph.conf...
Generating new minimal ceph.conf...
Restarting the monitor...
Setting mon public_network to 172.31.96.0/20
Wrote config to /etc/ceph/ceph.conf
Wrote keyring to /etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr not available, waiting (3/15)...
mgr not available, waiting (4/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /etc/ceph/ceph.pub
Adding key to root@localhost authorized_keys...
Adding host ceph01...
Deploying mon service with default placement...
Deploying mgr service with default placement...
Deploying crash service with default placement...
Deploying prometheus service with default placement...
Deploying grafana service with default placement...
Deploying node-exporter service with default placement...
Deploying alertmanager service with default placement...
Enabling the dashboard module...
Waiting for the mgr to restart...
Waiting for mgr epoch 9...
mgr epoch 9 is available
Generating a dashboard self-signed certificate...
Creating initial admin user...
Fetching dashboard port number...
Ceph Dashboard is now available at:

             URL: https://ceph01:8443/
            User: admin
        Password: 9ro6qdzyel

Enabling client.admin keyring and conf on hosts with "admin" label
Enabling autotune for osd_memory_target
You can access the Ceph CLI as following in case of multi-cluster or non-default config:

        sudo /usr/sbin/cephadm shell --fsid c32ff766-19f6-11ed-aa17-00163e005933 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

Or, if you are only running a single cluster on this host:

        sudo /usr/sbin/cephcadm shell 

Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/en/pacific/mgr/telemetry/

Bootstrap complete.
```

完成后记录以上了IP以及用户和密码，打开Ceph Dashboard并根据提示修改密码，打开后提示要激活统计模块。
如果错过使用命令
```cassandraql
如果Ceph Dashboard中错过了启用，也可以使用命令启用，命令是“ceph telemetry on --license sharing-1-0”。
```

如果忘记记录密码可以通过以下方法重置密码(将密码写入password文件中，通过命令导入密码)
```cassandraql
ceph dashboard ac-user-set-password admin -i password 
{"username": "admin", "password": "$2b$12$6oFrEpssXCzLnKTWQy5fM.YZwlHjn8CuQRdeSSJR9hBGgVuwGCxoa", "roles": ["administrator"], "name": null, "email": null, "lastUpdate": 1620495653, "enabled": true, "pwdExpirationDate": null, "pwdUpdateRequired": false}
```

##### 3.添加主机
在引导成功单节点Ceph群集后会引导程序会将public key的副本写入/etc/ceph/ceph.pub，在添加主机节点前需要讲该key分发到要加入群集的主机上

拷贝到ceph02
```cassandraql
ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph02
```

拷贝到ceph03
```cassandraql
ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph03
```

添加ceph 02 节点
```cassandraql
ceph orch host add ceph02 172.31.96.71
```
添加ceph 03 节点
```cassandraql
ceph orch host add ceph03 172.31.96.72
```

查看节点
```cassandraql
[root@ceph01 ~]# ceph orch host ls
HOST    ADDR          LABELS  STATUS  
ceph01  172.31.96.70  _admin          
ceph02  172.31.96.71                  
ceph03  172.31.96.72                  
3 hosts in cluster
```

##### 4.添加OSD
添加OSD需求满足以下所有条件：
- 设备必须没有分区。
- 设备不得具有任何LVM状态。
- 不得安装设备。
- 该设备不得包含文件系统。
- 该设备不得包含Ceph BlueStore OSD。
- 设备必须大于 5 GB。
添加OSD有2种方式，
1.为自动添加所有满足条件的OSD。
```
ceph orch apply osd --all-available-devices
```
2.为通过手工指定的方式添加OSD。
```cassandraql
ceph orch daemon add osd ceph1:/dev/sdb
```
本次使用第一种自动部署的方式，部署完成后查看设备列表，显示为NO就完成了。
```cassandraql
[root@ceph01 ~]# ceph orch device ls
HOST    PATH      TYPE  DEVICE ID              SIZE  AVAILABLE  REFRESHED  REJECT REASONS                                                 
ceph01  /dev/vdb  hdd   j6c3xd1a6qug5beqk71y  21.4G             9s ago     Insufficient space (<10 extents) on vgs, LVM detected, locked  
ceph02  /dev/vdb  hdd   j6c3xd1a6qug5beqk720  21.4G             14s ago    Insufficient space (<10 extents) on vgs, LVM detected, locked  
ceph03  /dev/vdb  hdd   j6c3xd1a6qug5beqk71z  21.4G             14s ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
```

#### 5.查看Ceph部署服务
```cassandraql
[root@ceph01 ~]# ceph -s
  cluster:
    id:     c32ff766-19f6-11ed-aa17-00163e005933
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum ceph01,ceph02,ceph03 (age 4m)
    mgr: ceph01.sdapbz(active, since 30m), standbys: ceph02.njhkgr
    osd: 3 osds: 3 up (since 96s), 3 in (since 114s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   15 MiB used, 60 GiB / 60 GiB avail
    pgs:     1 active+clean
 
```

打开 dashboard 看监控数据

#### 6.部署RGW
使用指定数量匹配模式部署。
```
ceph orch apply rgw rgw --placement=3
```
通过Service查看命令ceph orch ls查看该服务状态。
```cassandraql
[root@ceph01 ~]# ceph orch ls
NAME                       PORTS        RUNNING  REFRESHED  AGE  PLACEMENT  
alertmanager               ?:9093,9094      1/1  10s ago    40m  count:1    
crash                                       3/3  15s ago    40m  *          
grafana                    ?:3000           1/1  10s ago    40m  count:1    
mgr                                         2/2  14s ago    40m  count:2    
mon                                         3/5  15s ago    40m  count:5    
node-exporter              ?:9100           3/3  15s ago    40m  *          
osd.all-available-devices                     3  15s ago    9m   *          
prometheus                 ?:9095           1/1  10s ago    40m  count:1    
rgw.rgw                    ?:80             3/3  15s ago    30s  count:3
```
通过Deamon查看命令ceph orch ps查看该进程状态。
```cassandraql
[root@ceph01 ~]# ceph orch ps
NAME                   HOST    PORTS        STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID  
alertmanager.ceph01    ceph01  *:9093,9094  running (12m)    41s ago  39m    27.9M        -           ba2b418f427c  b6fbf7382136  
crash.ceph01           ceph01               running (39m)    41s ago  39m    7717k        -  16.2.10  0d668911f040  9660dd933bd2  
crash.ceph02           ceph02               running (14m)    44s ago  14m    9713k        -  16.2.10  0d668911f040  35cac443b75c  
crash.ceph03           ceph03               running (13m)    45s ago  13m    8632k        -  16.2.10  0d668911f040  3135f2560d98  
grafana.ceph01         ceph01  *:3000       running (38m)    41s ago  39m    67.9M        -  8.3.5    dad864ee21e9  7f47d88ad676  
mgr.ceph01.sdapbz      ceph01  *:9283       running (41m)    41s ago  41m     499M        -  16.2.10  0d668911f040  9e001eab3a53  
mgr.ceph02.njhkgr      ceph02  *:8443,9283  running (14m)    44s ago  14m     409M        -  16.2.10  0d668911f040  146058cf2aea  
mon.ceph01             ceph01               running (41m)    41s ago  41m     147M    2048M  16.2.10  0d668911f040  5896f4b2a014  
mon.ceph02             ceph02               running (14m)    44s ago  14m    85.3M    2048M  16.2.10  0d668911f040  7fd0c2c36613  
mon.ceph03             ceph03               running (13m)    45s ago  13m    82.1M    2048M  16.2.10  0d668911f040  db5a5b5039d5  
node-exporter.ceph01   ceph01  *:9100       running (38m)    41s ago  38m    16.5M        -           1dbe0e931976  2d7d68e3a0da  
node-exporter.ceph02   ceph02  *:9100       running (14m)    44s ago  14m    18.2M        -           1dbe0e931976  083f26d39f07  
node-exporter.ceph03   ceph03  *:9100       running (12m)    45s ago  12m    14.4M        -           1dbe0e931976  c3b21c79b2f4  
osd.0                  ceph03               running (9m)     45s ago   9m    40.4M    4096M  16.2.10  0d668911f040  2210df27e314  
osd.1                  ceph02               running (9m)     44s ago   9m    44.7M    4096M  16.2.10  0d668911f040  9ab8967c65b9  
osd.2                  ceph01               running (9m)     41s ago   9m    47.6M    4096M  16.2.10  0d668911f040  78a2000670f7  
prometheus.ceph01      ceph01  *:9095       running (12m)    41s ago  38m    69.2M        -           514e6a882f6e  06de63766e30  
rgw.rgw.ceph01.bvcnos  ceph01  *:80         running (58s)    41s ago  57s    43.7M        -  16.2.10  0d668911f040  96c4d8963d47  
rgw.rgw.ceph02.awygjp  ceph02  *:80         running (54s)    44s ago  53s    46.2M        -  16.2.10  0d668911f040  9d0bd0b0b060  
rgw.rgw.ceph03.iiixeq  ceph03  *:80         running (50s)    45s ago  49s    21.2M        -  16.2.10  0d668911f040  a5d6e8a8983a 
```
集成到dashboard
```cassandraql
[root@ceph01 ~]# radosgw-admin user create --uid=rgw --display-name=rgw --system

以下是输出结果

"keys": [
        {
            "user": "rgw",
            "access_key": "M0XRR80H4AGGE4PP0A5B",
            "secret_key": "Tbln48sfIceDGNill5muCrX0oMCHrQcl2oC9OURe"
        }
    ],{
    "user_id": "rgw",
    "display_name": "rgw",
    "email": "",
    "suspended": 0,
    "max_buckets": 1000,
    "subusers": [],
    "keys": [
        {
            "user": "rgw",
            "access_key": "43DTMZ3EY02B10QNTJSS",
            "secret_key": "4d3qzY1A7uqVk3LWVMsGJY0negbcxg95d8QJYuzi"
        }
    ],
    "swift_keys": [],
    "caps": [],
    "op_mask": "read, write, delete",
    "system": "true",
    "default_placement": "",
    "default_storage_class": "",
    "placement_tags": [],
    "bucket_quota": {
        "enabled": false,
        "check_on_raw": false,
        "max_size": -1,
        "max_size_kb": 0,
        "max_objects": -1
    },
    "user_quota": {
        "enabled": false,
        "check_on_raw": false,
        "max_size": -1,
        "max_size_kb": 0,
        "max_objects": -1
    },
    "temp_url_keys": [],
    "type": "rgw",
    "mfa_ids": []
}

```
查看 Dashboard 是否集成成功

#### 7.部署Cephfs
部署cephfs服务并创建cepfs，创建cephfs有两种方式，一种是使用的是ceph fs命令该命令会自动创建相应的池，另一种手工创建池并创建Service，下面方法任选一种。
方法一：
```cassandraql
ceph fs volume create cephfs --placement=3
```
方法二：
```cassandraql
#ceph osd pool create cephfs_data 32
#ceph osd pool create cephfs_metadata 32
#ceph fs new cephfs cephfs_metadata cephfs_data
#ceph orch apply mds cephfs --placement=3
```
我们采用方法一
查看Service状态。
```cassandraql
[root@ceph01 ~]# ceph fs volume create cephfs --placement=3
[root@ceph01 ~]# ceph orch ls
NAME                       PORTS        RUNNING  REFRESHED  AGE  PLACEMENT  
alertmanager               ?:9093,9094      1/1  9s ago     48m  count:1    
crash                                       3/3  13s ago    48m  *          
grafana                    ?:3000           1/1  9s ago     48m  count:1    
mds.cephfs                                  3/3  13s ago    29s  count:3    
mgr                                         2/2  13s ago    48m  count:2    
mon                                         3/5  13s ago    48m  count:5    
node-exporter              ?:9100           3/3  13s ago    48m  *          
osd.all-available-devices                     3  13s ago    17m  *          
prometheus                 ?:9095           1/1  9s ago     48m  count:1    
rgw.rgw                    ?:80             3/3  13s ago    8m   count:3   
```
查看Deamon状态。
```cassandraql
[root@ceph01 ~]# ceph orch ps
NAME                      HOST    PORTS        STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID  
alertmanager.ceph01       ceph01  *:9093,9094  running (20m)    59s ago  48m    29.5M        -           ba2b418f427c  b6fbf7382136  
crash.ceph01              ceph01               running (48m)    59s ago  48m    7587k        -  16.2.10  0d668911f040  9660dd933bd2  
crash.ceph02              ceph02               running (22m)    63s ago  22m    9667k        -  16.2.10  0d668911f040  35cac443b75c  
crash.ceph03              ceph03               running (21m)    63s ago  21m    8606k        -  16.2.10  0d668911f040  3135f2560d98  
grafana.ceph01            ceph01  *:3000       running (46m)    59s ago  47m    77.2M        -  8.3.5    dad864ee21e9  7f47d88ad676  
mds.cephfs.ceph01.uwclzo  ceph01               running (74s)    59s ago  74s    28.2M        -  16.2.10  0d668911f040  e7ead0c130a6  
mds.cephfs.ceph02.zqmpyt  ceph02               running (71s)    63s ago  70s    23.7M        -  16.2.10  0d668911f040  06ed8fd3bc1a  
mds.cephfs.ceph03.glufpl  ceph03               running (67s)    63s ago  67s    22.9M        -  16.2.10  0d668911f040  a3e497cb8a85  
mgr.ceph01.sdapbz         ceph01  *:9283       running (49m)    59s ago  49m     508M        -  16.2.10  0d668911f040  9e001eab3a53  
mgr.ceph02.njhkgr         ceph02  *:8443,9283  running (22m)    63s ago  22m     410M        -  16.2.10  0d668911f040  146058cf2aea  
mon.ceph01                ceph01               running (50m)    59s ago  50m     177M    2048M  16.2.10  0d668911f040  5896f4b2a014  
mon.ceph02                ceph02               running (22m)    63s ago  22m     115M    2048M  16.2.10  0d668911f040  7fd0c2c36613  
mon.ceph03                ceph03               running (21m)    63s ago  21m     111M    2048M  16.2.10  0d668911f040  db5a5b5039d5  
node-exporter.ceph01      ceph01  *:9100       running (47m)    59s ago  47m    17.4M        -           1dbe0e931976  2d7d68e3a0da  
node-exporter.ceph02      ceph02  *:9100       running (22m)    63s ago  22m    18.0M        -           1dbe0e931976  083f26d39f07  
node-exporter.ceph03      ceph03  *:9100       running (21m)    63s ago  21m    14.7M        -           1dbe0e931976  c3b21c79b2f4  
osd.0                     ceph03               running (17m)    63s ago  17m    69.7M    4096M  16.2.10  0d668911f040  2210df27e314  
osd.1                     ceph02               running (17m)    63s ago  17m    72.6M    4096M  16.2.10  0d668911f040  9ab8967c65b9  
osd.2                     ceph01               running (17m)    59s ago  17m    72.4M    4096M  16.2.10  0d668911f040  78a2000670f7  
prometheus.ceph01         ceph01  *:9095       running (20m)    59s ago  46m    73.6M        -           514e6a882f6e  06de63766e30  
rgw.rgw.ceph01.bvcnos     ceph01  *:80         running (9m)     59s ago   9m    73.3M        -  16.2.10  0d668911f040  96c4d8963d47  
rgw.rgw.ceph02.awygjp     ceph02  *:80         running (9m)     63s ago   9m    80.8M        -  16.2.10  0d668911f040  9d0bd0b0b060  
rgw.rgw.ceph03.iiixeq     ceph03  *:80         running (9m)     63s ago   8m    57.4M        -  16.2.10  0d668911f040  a5d6e8a8983a  
```

查看 Dashboard

#### 8.部署NFS
先创建nfs所需求的池。
```cassandraql
#ceph osd pool create ganesha_data 32
#ceph osd pool application enable ganesha_data nfs
```
部署nfs Service。
```cassandraql
#ceph orch apply nfs nfs ganesha_data --placement=3
```
查看Service状态。
```cassandraql
[root@ceph01 ~]# ceph orch ls
NAME                       PORTS        RUNNING  REFRESHED  AGE  PLACEMENT  
alertmanager               ?:9093,9094      1/1  8s ago     52m  count:1    
crash                                       3/3  13s ago    52m  *          
grafana                    ?:3000           1/1  8s ago     52m  count:1    
mds.cephfs                                  3/3  13s ago    4m   count:3    
mgr                                         2/2  13s ago    52m  count:2    
mon                                         3/5  13s ago    52m  count:5    
nfs.nfs                                     3/3  13s ago    36s  count:3    
node-exporter              ?:9100           3/3  13s ago    52m  *          
osd.all-available-devices                     3  13s ago    21m  *          
prometheus                 ?:9095           1/1  8s ago     52m  count:1    
rgw.rgw                    ?:80             3/3  13s ago    12m  count:3  
```

查看Deamon状态。
```cassandraql
[root@ceph01 ~]# ceph orch ls
NAME                       PORTS        RUNNING  REFRESHED  AGE  PLACEMENT  
alertmanager               ?:9093,9094      1/1  8s ago     52m  count:1    
crash                                       3/3  13s ago    52m  *          
grafana                    ?:3000           1/1  8s ago     52m  count:1    
mds.cephfs                                  3/3  13s ago    4m   count:3    
mgr                                         2/2  13s ago    52m  count:2    
mon                                         3/5  13s ago    52m  count:5    
nfs.nfs                                     3/3  13s ago    36s  count:3    
node-exporter              ?:9100           3/3  13s ago    52m  *          
osd.all-available-devices                     3  13s ago    21m  *          
prometheus                 ?:9095           1/1  8s ago     52m  count:1    
rgw.rgw                    ?:80             3/3  13s ago    12m  count:3    

```

#### 9.部署iSCSi
创建iscsi所需求的池。
```cassandraql
#ceph osd pool create  iscsi_pool 16 16
#ceph osd pool application enable iscsi_pool iscsi
```

部署iscsi我们换YAM方式
```cassandraql
#vi iscsi.yaml
service_type: iscsi
service_id: gw
placement:
  hosts:
    - ceph01
    - ceph02
    - ceph03
spec:
  pool: iscsi_pool
  trusted_ip_list: "172.31.96.70,172.31.96.71,172.31.96.72"
  api_user: admin
  api_password: admin
  api_secure: false

```
通过apply命令部署，cephadm也是声明式的，所以如果想修改配置参数只需要直接修改YAML文件。

```cassandraql
[root@ceph01 ~]# ceph orch apply -i iscsi.yaml
Scheduled iscsi.gw update...
```
查看Service状态。
```cassandraql
[root@ceph01 ~]# ceph orch ls
NAME                       PORTS        RUNNING  REFRESHED  AGE  PLACEMENT             
alertmanager               ?:9093,9094      1/1  9s ago     67m  count:1               
crash                                       3/3  21s ago    67m  *                     
grafana                    ?:3000           1/1  9s ago     67m  count:1               
iscsi.gw                                    3/3  21s ago    40s  ceph01;ceph02;ceph03  
mds.cephfs                                  3/3  21s ago    19m  count:3               
mgr                                         2/2  20s ago    67m  count:2               
mon                                         3/5  21s ago    67m  count:5               
nfs.nfs                                     3/3  21s ago    16m  count:3               
node-exporter              ?:9100           3/3  21s ago    67m  *                     
osd.all-available-devices                     3  21s ago    36m  *                     
prometheus                 ?:9095           1/1  9s ago     67m  count:1               
rgw.rgw                    ?:80             3/3  21s ago    27m  count:3   
```
查看Deamon状态。
```cassandraql
[root@ceph01 ~]# ceph orch ps
NAME                       HOST    PORTS        STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID  
alertmanager.ceph01        ceph01  *:9093,9094  running (39m)    33s ago  67m    35.2M        -           ba2b418f427c  b6fbf7382136  
crash.ceph01               ceph01               running (67m)    33s ago  67m    7386k        -  16.2.10  0d668911f040  9660dd933bd2  
crash.ceph02               ceph02               running (41m)    44s ago  41m    10.5M        -  16.2.10  0d668911f040  35cac443b75c  
crash.ceph03               ceph03               running (40m)    45s ago  40m    10.8M        -  16.2.10  0d668911f040  3135f2560d98  
grafana.ceph01             ceph01  *:3000       running (65m)    33s ago  66m    80.2M        -  8.3.5    dad864ee21e9  7f47d88ad676  
iscsi.gw.ceph01.mqjxqu     ceph01               running (60s)    33s ago  59s    76.1M        -  3.5      0d668911f040  e712bdbb2d80  
iscsi.gw.ceph02.xeggjo     ceph02               running (56s)    44s ago  56s    52.9M        -  3.5      0d668911f040  04c7f5263155  
iscsi.gw.ceph03.xykglu     ceph03               running (52s)    45s ago  52s    76.9M        -  3.5      0d668911f040  59cb4c7bbfd5  
mds.cephfs.ceph01.uwclzo   ceph01               running (20m)    33s ago  20m    27.6M        -  16.2.10  0d668911f040  e7ead0c130a6  
mds.cephfs.ceph02.zqmpyt   ceph02               running (20m)    44s ago  20m    26.8M        -  16.2.10  0d668911f040  06ed8fd3bc1a  
mds.cephfs.ceph03.glufpl   ceph03               running (20m)    45s ago  20m    26.5M        -  16.2.10  0d668911f040  a3e497cb8a85  
mgr.ceph01.sdapbz          ceph01  *:9283       running (68m)    33s ago  68m     495M        -  16.2.10  0d668911f040  9e001eab3a53  
mgr.ceph02.njhkgr          ceph02  *:8443,9283  running (41m)    44s ago  41m     411M        -  16.2.10  0d668911f040  146058cf2aea  
mon.ceph01                 ceph01               running (68m)    33s ago  69m     237M    2048M  16.2.10  0d668911f040  5896f4b2a014  
mon.ceph02                 ceph02               running (41m)    44s ago  41m     164M    2048M  16.2.10  0d668911f040  7fd0c2c36613  
mon.ceph03                 ceph03               running (40m)    45s ago  40m     146M    2048M  16.2.10  0d668911f040  db5a5b5039d5  
nfs.nfs.0.0.ceph01.ijxnwe  ceph01  *:2049       running (16m)    33s ago  16m    72.4M        -  3.5      0d668911f040  f9b0609cbe7b  
nfs.nfs.1.0.ceph02.pedadk  ceph02  *:2049       running (16m)    44s ago  16m    75.8M        -  3.5      0d668911f040  54d58b785ed6  
nfs.nfs.2.0.ceph03.daqdkw  ceph03  *:2049       running (16m)    45s ago  16m    73.0M        -  3.5      0d668911f040  c1bebe7782d4  
node-exporter.ceph01       ceph01  *:9100       running (66m)    33s ago  66m    17.7M        -           1dbe0e931976  2d7d68e3a0da  
node-exporter.ceph02       ceph02  *:9100       running (41m)    44s ago  41m    18.3M        -           1dbe0e931976  083f26d39f07  
node-exporter.ceph03       ceph03  *:9100       running (40m)    45s ago  40m    15.5M        -           1dbe0e931976  c3b21c79b2f4  
osd.0                      ceph03               running (36m)    45s ago  36m    77.8M    4096M  16.2.10  0d668911f040  2210df27e314  
osd.1                      ceph02               running (36m)    44s ago  36m    80.1M    4096M  16.2.10  0d668911f040  9ab8967c65b9  
osd.2                      ceph01               running (36m)    33s ago  36m    79.3M    4096M  16.2.10  0d668911f040  78a2000670f7  
prometheus.ceph01          ceph01  *:9095       running (39m)    33s ago  65m    97.3M        -           514e6a882f6e  06de63766e30  
rgw.rgw.ceph01.bvcnos      ceph01  *:80         running (28m)    33s ago  28m    61.8M        -  16.2.10  0d668911f040  96c4d8963d47  
rgw.rgw.ceph02.awygjp      ceph02  *:80         running (28m)    44s ago  28m    81.6M        -  16.2.10  0d668911f040  9d0bd0b0b060  
rgw.rgw.ceph03.iiixeq      ceph03  *:80         running (27m)    45s ago  27m    60.0M        -  16.2.10  0d668911f040  a5d6e8a8983a  
```
